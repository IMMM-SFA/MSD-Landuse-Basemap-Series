{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import rasterstats as rs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to intergers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input and output directories\n",
    "input_folder = r\".\\LUH2\\Clipped\"\n",
    "output_folder = r\".\\LUH2\\Int_Folder\"\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# List all raster files in the input folder\n",
    "raster_files = [f for f in os.listdir(input_folder) if f.endswith('.tif')]\n",
    "\n",
    "# Loop through each raster file and convert to integer type\n",
    "for raster_file in raster_files:\n",
    "    input_raster = os.path.join(input_folder, raster_file)\n",
    "    output_raster = os.path.join(output_folder, raster_file)\n",
    "\n",
    "    # Open the raster file\n",
    "    with rasterio.open(input_raster) as src:\n",
    "        # Read the data\n",
    "        data = src.read(1)  # Read the first band\n",
    "\n",
    "        # Get the NoData value\n",
    "        nodata_value = src.nodata\n",
    "\n",
    "        # Handle NoData values\n",
    "        if nodata_value is not None:\n",
    "            data = np.where(data == nodata_value, -9999, data)\n",
    "\n",
    "        int_data = data.astype(np.int8)  # Change np.int32 to np.int64 if needed\n",
    "\n",
    "        # Define metadata for the output raster\n",
    "        metadata = src.meta.copy()\n",
    "        metadata.update({\n",
    "            'dtype': 'int32',  # Change to 'int64' if needed\n",
    "            'count': 1,\n",
    "            'nodata': -15  # Set the NoData value for the output raster\n",
    "        })\n",
    "\n",
    "        # Write the converted data to a new raster file\n",
    "        with rasterio.open(output_raster, 'w', **metadata) as dst:\n",
    "            dst.write(int_data, 1)\n",
    "\n",
    "    print(f\"Converted {input_raster} to {output_raster}\")\n",
    "\n",
    "print(\"All raster files have been converted and plotted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Zonal Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input and output directories\n",
    "input_folder = r\".\\LUH2\\Int_Folder\"\n",
    "point_feature_class = r\".\\CLM_Testing.gdb\\Point_LUH2_XyToPoint.shp\"\n",
    "output_folder = r\".\\LUH2\\Zonal_Stats\"\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Load the point feature class as a GeoDataFrame\n",
    "gdf = gpd.read_file(point_feature_class)\n",
    "\n",
    "# Ensure the dataset has a unique ID column\n",
    "point_id_field = \"pointid\"  # Modify this if the ID field is different\n",
    "\n",
    "# List all TIFF files in the input folder\n",
    "tiff_files = [f for f in os.listdir(input_folder) if f.endswith('.tif')]\n",
    "\n",
    "# Loop through each TIFF file and calculate zonal statistics\n",
    "for tiff_file in tiff_files:\n",
    "    input_tiff = os.path.join(input_folder, tiff_file)\n",
    "    \n",
    "    # Compute zonal statistics\n",
    "    zonal_stats = rs.zonal_stats(\n",
    "        gdf, input_tiff, stats=[\"max\"], geojson_out=True\n",
    "    )\n",
    "\n",
    "    # Convert results to a DataFrame\n",
    "    df = pd.DataFrame([\n",
    "        {\n",
    "            point_id_field: feature[\"properties\"][point_id_field],\n",
    "            \"MaxValue\": feature[\"properties\"][\"max\"]\n",
    "        }\n",
    "        for feature in zonal_stats\n",
    "    ])\n",
    "\n",
    "    # Define the output file path (CSV instead of DBF)\n",
    "    output_csv = os.path.join(output_folder, f\"ZonalStats_{os.path.splitext(tiff_file)[0]}.csv\")\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "    print(f\"Zonal statistics calculated for {input_tiff} and saved to {output_csv}\")\n",
    "\n",
    "print(\"All zonal statistics have been calculated.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing the DBF files\n",
    "output_folder = r\".\\LUH2\\Zonal_Stats\"\n",
    "merged_output_folder = r\".\\LUH2\\MSE_calculation\"\n",
    "\n",
    "# Create necessary output directories\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "os.makedirs(merged_output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "# Initialize an empty list to hold DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Loop through all CSV files in the output directory\n",
    "for filename in os.listdir(output_folder):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(os.path.join(output_folder, filename))\n",
    "        \n",
    "        # Get the base name of the file (without extension)\n",
    "        base_name = os.path.splitext(filename)[0]\n",
    "        \n",
    "        # Rename columns that contain 'MaxValue' to include the base name\n",
    "        df.columns = df.columns.str.replace('MaxValue', f'{base_name}', case=False)\n",
    "        \n",
    "        # Remove the specific substring from column names\n",
    "        df.columns = df.columns.str.replace('ZonalStats_Clipped_Multiplied_states_', '', case=False)\n",
    "        \n",
    "        # Drop columns that contain 'COUNT' or 'Area'\n",
    "        df = df.loc[:, ~df.columns.str.contains('COUNT|Area', case=False)]\n",
    "        \n",
    "        # Check for duplicate columns and keep the first occurrence\n",
    "        df = df.loc[:, ~df.columns.duplicated(keep='first')]\n",
    "\n",
    "        dataframes.append(df)\n",
    "\n",
    "# Merge all DataFrames on 'pointid'\n",
    "if dataframes:\n",
    "    merged_df = dataframes[0]\n",
    "    for df in dataframes[1:]:\n",
    "        merged_df = pd.merge(merged_df, df, on='pointid', how='outer', suffixes=('', '_dup'))\n",
    "        \n",
    "        # Remove duplicate columns that may have been created during the merge\n",
    "        merged_df = merged_df.loc[:, ~merged_df.columns.str.endswith('_dup')]\n",
    "\n",
    "    # Save the merged DataFrame to a new CSV file in the merged output directory\n",
    "    output_file_path = os.path.join(merged_output_folder, 'merged_output.csv')\n",
    "    merged_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"Merging and cleaning completed. Output saved as '{output_file_path}'.\")\n",
    "else:\n",
    "    print(\"No CSV files found to merge.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing cell size set to: 30.0000016640515\n",
      "Tabulate Area completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "workspace = r\"./\"\n",
    "gdb_path = os.path.join(workspace, \"CLM_Testing.gdb\")\n",
    "input_raster = os.path.join(gdb_path, \"CellSta_MSD_2008.tif\")  # Assuming the raster is extracted as a .tif\n",
    "vector_layer = os.path.join(gdb_path, \"Point_LUH2_ReProjectRaster.shp\")  # Assuming it's a Shapefile\n",
    "output_csv = os.path.join(workspace, \"Tabulate_LUH_MSD_Classes.csv\")\n",
    "\n",
    "# Step 1: Get the cell size of the raster\n",
    "try:\n",
    "    with rasterio.open(input_raster) as raster:\n",
    "        cell_size = raster.res[0]  # Assuming square pixels\n",
    "        print(f\"Processing cell size set to: {cell_size}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading raster file: {str(e)}\")\n",
    "    exit()\n",
    "\n",
    "# Step 2: Perform Tabulate Area (Zonal Statistics)\n",
    "try:\n",
    "    # Load the vector layer (geospatial points or polygons)\n",
    "    gdf = gpd.read_file(vector_layer)\n",
    "\n",
    "    # Compute zonal statistics (counts number of pixels per category)\n",
    "    stats = zonal_stats(gdf, input_raster, stats=[\"count\"], categorical=True, geojson_out=True)\n",
    "\n",
    "    # Convert results to a DataFrame\n",
    "    tabulate_df = pd.DataFrame([\n",
    "        {**{\"ID\": feature[\"properties\"][\"ID\"]}, **feature[\"properties\"]}\n",
    "        for feature in stats\n",
    "    ])\n",
    "\n",
    "    # Save the output table as CSV\n",
    "    tabulate_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Tabulate Area completed successfully. Output saved to {output_csv}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during zonal statistics calculation: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "output_folder = r\".\\LUH2\\MSE calculation\"\n",
    "exported_table = os.path.join(output_folder, \"Tabulate_LUH_MSD_Classes.csv.csv\")\n",
    "existing_table = os.path.join(output_folder, \"merged_output.csv\")\n",
    "merged_output = os.path.join(output_folder, \"final_merged_output_V_01_25.csv\")\n",
    "\n",
    "# Check if both tables exist\n",
    "if os.path.exists(exported_table) and os.path.exists(existing_table):\n",
    "    # Load the tables as pandas DataFrames\n",
    "    df_exported = pd.read_csv(exported_table)\n",
    "    df_existing = pd.read_csv(existing_table)\n",
    "\n",
    "    # Merge the tables on 'pointid' (left) and 'Value' (right)\n",
    "    df_merged = pd.merge(df_existing, df_exported, left_on='pointid', right_on='VALUE', how='outer')\n",
    "\n",
    "    # Save the merged table to a new CSV file\n",
    "    df_merged.to_csv(merged_output, index=False)\n",
    "    print(f\"Tables merged successfully into {merged_output}\")\n",
    "else:\n",
    "    print(f\"One or both tables not found. Ensure both '{exported_table}' and '{existing_table}' exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "output_folder = r\".\\LUH2\\MSE calculation\"\n",
    "merged_file = os.path.join(output_folder, \"final_merged_output_V_01_25.csv\")\n",
    "classification_file = os.path.join(output_folder, \"Raw Classification.csv\")\n",
    "\n",
    "# Check if the files exist\n",
    "if os.path.exists(merged_file) and os.path.exists(classification_file):\n",
    "    # Load the merged file and classification file into DataFrames\n",
    "    df_merged = pd.read_csv(merged_file)\n",
    "    df_classification = pd.read_csv(classification_file)\n",
    "    \n",
    "    # Extract the 'MSD Value' and 'Layer Name' columns from classification.csv\n",
    "    # The 'MSD Value' will be used to map to 'VALUE_' columns in the merged file\n",
    "    value_to_layer = {}\n",
    "    \n",
    "    # Iterate through rows in classification.csv to create the mapping from MSD Value to Layer Name\n",
    "    for idx, row in df_classification.iterrows():\n",
    "        msd_value = row['MSD Val']\n",
    "        layer_name = row['Layer Name']\n",
    "        value_to_layer[msd_value] = layer_name\n",
    "    \n",
    "    # Identify the 'VALUE_' columns in the merged DataFrame\n",
    "    value_columns = [col for col in df_merged.columns if 'VALUE_' in col]\n",
    "    \n",
    "    # Create a dictionary to map the 'VALUE_' columns to the corresponding Layer Name\n",
    "    value_column_mapping = {}\n",
    "    \n",
    "    for value_column in value_columns:\n",
    "        # Extract the number from the 'VALUE_' column (e.g., 'VALUE_1' -> 1)\n",
    "        value_number = int(value_column.split('_')[1])\n",
    "        \n",
    "        # Map the MSD Value to the corresponding Layer Name from classification.csv\n",
    "        if value_number in value_to_layer:\n",
    "            value_column_mapping[value_column] = value_to_layer[value_number]\n",
    "    \n",
    "    # Rename the columns in df_merged using the mapping\n",
    "    df_merged.rename(columns=value_column_mapping, inplace=True)\n",
    "\n",
    "    # Save the updated DataFrame back to CSV\n",
    "    df_merged.to_csv(merged_file, index=False)\n",
    "    print(f\"Column names updated successfully in {merged_file}\")\n",
    "else:\n",
    "    print(f\"One or both files '{merged_file}' and '{classification_file}' do not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert csv file to excel, and calculate proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define file paths\n",
    "csv_file = r\".\\LUH2\\MSE calculation\\final_merged_output.csv\"\n",
    "excel_file = r\".\\LUH2\\MSE calculation\\output.xlsx\"\n",
    "\n",
    "# Step 1: Convert CSV to Excel\n",
    "if os.path.exists(csv_file):\n",
    "    # Load the CSV into a DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Save it as an Excel file\n",
    "    with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:\n",
    "        df.to_excel(writer, sheet_name='final_merged_output', index=False)\n",
    "    print(f\"CSV converted to Excel and saved as '{excel_file}'.\")\n",
    "else:\n",
    "    print(f\"CSV file '{csv_file}' does not exist.\")\n",
    "    exit()\n",
    "\n",
    "# Step 2: Calculate proportions per cell starting from column 'O'\n",
    "# Reload the Excel file with the added tab\n",
    "df = pd.read_excel(excel_file, sheet_name='final_merged_output')\n",
    "\n",
    "# Identify columns starting from column 'O'\n",
    "start_column = 'Irrigated Temperate Corn'\n",
    "start_index = df.columns.get_loc(start_column)\n",
    "\n",
    "# Calculate proportions for each cell in columns starting from 'O'\n",
    "proportions = df.iloc[:, start_index:].div(df.iloc[:, start_index:].sum(axis=1), axis=0)\n",
    "\n",
    "# Add proportions to the DataFrame with appropriate headers\n",
    "proportion_columns = [f\"{col}\" for col in df.columns[start_index:]]\n",
    "proportions.columns = proportion_columns\n",
    "\n",
    "# Step 3: Save to a new tab in the Excel file\n",
    "with pd.ExcelWriter(excel_file, engine='openpyxl', mode='a') as writer:\n",
    "    proportions.to_excel(writer, sheet_name='Proportions', index=False)\n",
    "print(f\"Proportions added as a new tab in '{excel_file}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spatial Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "output_excel = r\".\\LUH2\\MSE calculation\\output.xlsx\"\n",
    "raw_classification_excel = r\".\\LUH2\\MSE calculation\\Raw Classification.xlsx\"\n",
    "\n",
    "# Step 1: Load the Proportions tab from output.xlsx\n",
    "proportions_df = pd.read_excel(output_excel, sheet_name='Proportions')\n",
    "\n",
    "# Step 2: Load the Raw Classification table\n",
    "classification_df = pd.read_excel(raw_classification_excel)\n",
    "\n",
    "# Step 3: Prepare the mapping\n",
    "classification_df['Layer Name'] = classification_df['Layer Name'].str.strip()\n",
    "classification_df['LUH2 Classification'] = classification_df['LUH2 Classification'].str.strip()\n",
    "\n",
    "# Melt the Proportions DataFrame for easier aggregation\n",
    "proportions_long = proportions_df.melt(ignore_index=False, var_name='Layer Name', value_name='Proportion')\n",
    "\n",
    "# Merge with the classification mapping\n",
    "merged_df = pd.merge(proportions_long.reset_index(), classification_df, on='Layer Name', how='inner')\n",
    "\n",
    "# Step 4: Aggregate proportions per cell grouped by LUH2 Classification\n",
    "aggregated_df = merged_df.groupby(['index', 'LUH2 Classification'])['Proportion'].sum().unstack(fill_value=0)\n",
    "\n",
    "# Step 5: Handle columns with \"_Non_Forested\" and \"Forested_\"\n",
    "# Identify and combine Non_Forested columns\n",
    "non_forested_cols = [col for col in aggregated_df.columns if 'Non_Forested_' in col]\n",
    "aggregated_df['Non_Forested'] = aggregated_df[non_forested_cols].sum(axis=1)\n",
    "aggregated_df.drop(non_forested_cols, axis=1, inplace=True)\n",
    "\n",
    "# Identify and combine Forested columns\n",
    "forested_cols = [col for col in aggregated_df.columns if 'Forested_' in col]\n",
    "aggregated_df['Forested'] = aggregated_df[forested_cols].sum(axis=1)\n",
    "aggregated_df.drop(forested_cols, axis=1, inplace=True)\n",
    "\n",
    "# Step 6: Save to the output Excel file\n",
    "with pd.ExcelWriter(output_excel, engine='openpyxl', mode='a') as writer:\n",
    "    aggregated_df.to_excel(writer, sheet_name='Aggregation')\n",
    "\n",
    "print(f\"Aggregation tab created successfully in '{output_excel}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path\n",
    "output_excel = r\".\\LUH2\\MSE calculation\\output.xlsx\"\n",
    "\n",
    "# Step 1: Load the Aggregation and final_merged_output tabs\n",
    "aggregation_df = pd.read_excel(output_excel, sheet_name='Aggregation')\n",
    "final_merged_output_df = pd.read_excel(output_excel, sheet_name='final_merged_output')\n",
    "\n",
    "# Step 2: Preprocess final_merged_output\n",
    "# Combine all columns with \"_Non_Forested\" into a single column named \"Non_Forested\"\n",
    "non_forested_columns = [col for col in final_merged_output_df.columns if \"_Non_Forested\" in col]\n",
    "final_merged_output_df['Non_Forested'] = final_merged_output_df[non_forested_columns].sum(axis=1)\n",
    "final_merged_output_df.drop(columns=non_forested_columns, inplace=True)\n",
    "\n",
    "# Combine all columns with \"Forested_\" excluding those with \"Non_Forested_\" into a single column named \"Forested\"\n",
    "forested_columns = [col for col in final_merged_output_df.columns if \"Forested_\" in col and \"Non_Forested_\" not in col]\n",
    "final_merged_output_df['Forested'] = final_merged_output_df[forested_columns].sum(axis=1)\n",
    "final_merged_output_df.drop(columns=forested_columns, inplace=True)\n",
    "\n",
    "# Step 3: Ensure both DataFrames have matching columns\n",
    "# Find common columns between Aggregation and processed final_merged_output\n",
    "common_columns = list(set(aggregation_df.columns).intersection(final_merged_output_df.columns))\n",
    "\n",
    "# Step 4: Perform the MSE calculation\n",
    "# Subset the DataFrames to only include the common columns\n",
    "aggregation_subset = aggregation_df[common_columns]\n",
    "final_merged_subset = final_merged_output_df[common_columns]\n",
    "\n",
    "# Apply the MSE formula: (Aggregation - (final_merged_output / 100))^2\n",
    "mse_df = (aggregation_subset - (final_merged_subset / 100)) ** 2\n",
    "\n",
    "# Step 4: Sum all columns per row, divide by 100, and add the result to a new column\n",
    "mse_df['MSE_Sum_Per_Row'] = mse_df.sum(axis=1) / 100\n",
    "\n",
    "# Step 5: Save the MSE results to a new tab in output.xlsx\n",
    "with pd.ExcelWriter(output_excel, engine='openpyxl', mode='a') as writer:\n",
    "    mse_df.to_excel(writer, sheet_name='MSE calculation', index=False)\n",
    "\n",
    "print(f\"MSE calculation tab created successfully in '{output_excel}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path to the Excel file\n",
    "output_excel = r\".\\LUH2\\MSE calculation\\output.xlsx\"\n",
    "output_csv = r\".\\LUH2\\MSE calculation\\pointid_mse.csv\"\n",
    "\n",
    "# Step 1: Load the relevant tabs from the Excel file\n",
    "final_merged_output_df = pd.read_excel(output_excel, sheet_name='final_merged_output')\n",
    "mse_calculation_df = pd.read_excel(output_excel, sheet_name='MSE calculation')\n",
    "\n",
    "# Step 2: Extract the \"point id\" column and the \"MSE_Sum_Per_Row\" column\n",
    "# Assuming \"point id\" exists in final_merged_output_df and \"MSE_Sum_Per_Row\" in mse_calculation_df\n",
    "if 'pointid' in final_merged_output_df.columns:\n",
    "    point_id_column = final_merged_output_df['pointid']\n",
    "else:\n",
    "    raise ValueError(\"'point id' column not found in final_merged_output tab.\")\n",
    "\n",
    "if 'MSE_Sum_Per_Row' in mse_calculation_df.columns:\n",
    "    mse_sum_column = mse_calculation_df['MSE_Sum_Per_Row']\n",
    "else:\n",
    "    raise ValueError(\"'MSE_Sum_Per_Row' column not found in MSE calculation tab.\")\n",
    "\n",
    "# Step 3: Combine the columns into a new DataFrame\n",
    "output_df = pd.DataFrame({\n",
    "    'point id': point_id_column,\n",
    "    'MSE_Sum_Per_Row': mse_sum_column\n",
    "})\n",
    "\n",
    "# Step 4: Save the new DataFrame to a CSV file\n",
    "output_df.to_csv(output_csv, index=False)\n",
    "\n",
    "output_csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
